import torch
import torch.nn as nn
import torch.optim as optim
from torchvision import models

# --- モデル準備（ImageNetから転移学習） ---
model = models.efficientnet_b0(pretrained=True)
model.classifier[1] = nn.Linear(model.classifier[1].in_features, 288)

# --- 全層を一旦凍結 ---
for param in model.parameters():
    param.requires_grad = False

# --- 出力層だけを解凍（最初） ---
for param in model.classifier.parameters():
    param.requires_grad = True

# --- Optimizer ---
optimizer = optim.SGD(filter(lambda p: p.requires_grad, model.parameters()),
                      lr=0.01, momentum=0.9, weight_decay=1e-4)

# --- CosineAnnealingLRスケジューラ（150ep想定） ---
scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=150, eta_min=1e-5)

# --- 解凍スケジュール ---
def unfreeze_schedule(model, epoch):
    if epoch == 15:
        for param in model.features[6].parameters():
            param.requires_grad = True
    elif epoch == 30:
        for param in model.features[5].parameters():
            param.requires_grad = True
    elif epoch == 45:
        for param in model.features[4].parameters():
            param.requires_grad = True
    elif epoch == 60:
        for param in model.features[0:4].parameters():
            param.requires_grad = True  # 全層解凍


def unfreeze_schedule(model, epoch):
    if epoch == 20:
        for param in model.features[8].parameters():  # 最後のConvブロック
            param.requires_grad = True
    elif epoch == 40:
        for param in model.features[6].parameters():
            param.requires_grad = True
        for param in model.features[7].parameters():
            param.requires_grad = True
    elif epoch == 60:
        for param in model.features[4].parameters():
            param.requires_grad = True
        for param in model.features[5].parameters():
            param.requires_grad = True
    elif epoch == 80:
        for i in range(0, 4):  # features[0]〜[3]
            for param in model.features[i].parameters():
                param.requires_grad = True

# --- 学習ループ ---
for epoch in range(150):
    unfreeze_schedule(model, epoch)

    model.train()
    for images, labels in train_loader:
        optimizer.zero_grad()
        outputs = model(images)
        loss = criterion(outputs, labels)
        loss.backward()
        optimizer.step()

    scheduler.step()


import torch
import torch.nn as nn
import torch.optim as optim
from torchvision import models

# --- 学習済みモデルロード（284クラス） ---
model = models.efficientnet_b0(pretrained=False)
model.classifier[1] = nn.Linear(model.classifier[1].in_features, 284)
model.load_state_dict(torch.load("sushi_284_model.pth"))  # パスは適宜変更

# --- 288クラスに出力層を更新 ---
model.classifier[1] = nn.Linear(model.classifier[1].in_features, 288)

# --- 全層凍結 ---
for param in model.parameters():
    param.requires_grad = False

# --- 出力層だけを解凍（最初） ---
for param in model.classifier.parameters():
    param.requires_grad = True

# --- Optimizer ---
optimizer = optim.SGD(filter(lambda p: p.requires_grad, model.parameters()),
                      lr=0.01, momentum=0.9, weight_decay=1e-4)

# --- CosineAnnealingLR（120エポック想定） ---
scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=120, eta_min=1e-5)

# --- 段階解凍（features[3]〜[6]まで） ---
def unfreeze_schedule(model, epoch):
    if epoch == 15:
        for param in model.features[6].parameters():
            param.requires_grad = True
    elif epoch == 30:
        for param in model.features[5].parameters():
            param.requires_grad = True
    elif epoch == 45:
        for param in model.features[4].parameters():
            param.requires_grad = True
    elif epoch == 60:
        for param in model.features[3].parameters():
            param.requires_grad = True
    # [0]〜[2]は固定（再学習しない）

def unfreeze_schedule(model, epoch):
    if epoch == 20:
        for param in model.features[8].parameters():
            param.requires_grad = True
    elif epoch == 40:
        for param in model.features[6].parameters():
            param.requires_grad = True
        for param in model.features[7].parameters():
            param.requires_grad = True
    elif epoch == 60:
        for param in model.features[4].parameters():
            param.requires_grad = True
        for param in model.features[5].parameters():
            param.requires_grad = True
    elif epoch == 80:
        # 部分モードの場合、初期層も微調整
        for i in range(0, 4):
            for param in model.features[i].parameters():
                param.requires_grad = True

# --- 学習ループ ---
for epoch in range(120):
    unfreeze_schedule(model, epoch)

    model.train()
    for images, labels in train_loader:
        optimizer.zero_grad()
        outputs = model(images)
        loss = criterion(outputs, labels)
        loss.backward()
        optimizer.step()

    scheduler.step()

def unfreeze_schedule(model, epoch, mode='full'):
    if epoch == 20:
        for param in model.features[8].parameters():
            param.requires_grad = True
    elif epoch == 40:
        for param in model.features[6].parameters():
            param.requires_grad = True
        for param in model.features[7].parameters():
            param.requires_grad = True
    elif epoch == 60:
        for param in model.features[4].parameters():
            param.requires_grad = True
        for param in model.features[5].parameters():
            param.requires_grad = True
    elif epoch == 80:
        if mode == 'partial':
            # ✅ 部分モード：初期層も微調整
            for i in range(0, 4):
                for param in model.features[i].parameters():
                    param.requires_grad = True
        else:
            # ❌ 通常モード：初期層は引き続き凍結（明示的に再凍結しておく）
            for i in range(0, 4):
                for param in model.features[i].parameters():
                    param.requires_grad = False

