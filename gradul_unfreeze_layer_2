import os
import torch
import torch.nn as nn
import torch.optim as optim
from torchvision import datasets, transforms, models
from torch.utils.data import DataLoader
from torch.optim.lr_scheduler import StepLR

# ---------- „É¢„Éá„É´ÊßãÁØâ ----------
def build_model(num_classes=290):
    model = models.efficientnet_b0(pretrained=True)
    in_features = model.classifier[1].in_features
    model.classifier[1] = nn.Linear(in_features, num_classes)
    return model

# ---------- ÊÆµÈöéÁöÑ„Å™Ëß£Âáç ----------
def unfreeze_layers(model, epoch):
    for param in model.parameters():
        param.requires_grad = False
    for param in model.classifier.parameters():
        param.requires_grad = True

    if epoch >= 10:
        for name, param in model.named_parameters():
            if any(f"features.{i}" in name for i in [7, 8]):
                param.requires_grad = True
    if epoch >= 20:
        for name, param in model.named_parameters():
            if any(f"features.{i}" in name for i in [5, 6]):
                param.requires_grad = True
    if epoch >= 30:
        for name, param in model.named_parameters():
            if any(f"features.{i}" in name for i in [3, 4]):
                param.requires_grad = True
    if epoch >= 40:
        for name, param in model.named_parameters():
            if any(f"features.{i}" in name for i in [0, 1, 2]):
                param.requires_grad = True

# ---------- Â±§„Åî„Å®„ÅÆÂ≠¶ÁøíÁéáË®≠ÂÆö ----------
def get_optimizer(model):
    param_groups = [
        {'params': model.classifier.parameters(), 'lr': 1e-3},
        {'params': model.features[7].parameters(), 'lr': 1e-4},
        {'params': model.features[6].parameters(), 'lr': 1e-4},
        {'params': model.features[5].parameters(), 'lr': 5e-5},
        {'params': model.features[4].parameters(), 'lr': 1e-5},
        {'params': model.features[0].parameters(), 'lr': 1e-6},
    ]
    return optim.AdamW(param_groups)

# ---------- „Éá„Éº„Çø„É≠„Éº„ÉÄ„Éº ----------
def get_dataloaders(data_dir, batch_size=32, num_workers=8):
    transform = transforms.Compose([
        transforms.Resize((224, 224)),
        transforms.RandomHorizontalFlip(),
        transforms.ToTensor(),
    ])
    train_set = datasets.ImageFolder(os.path.join(data_dir, 'train'), transform=transform)
    val_set = datasets.ImageFolder(os.path.join(data_dir, 'val'), transform=transform)

    train_loader = DataLoader(train_set, batch_size=batch_size, shuffle=True, num_workers=num_workers, pin_memory=True)
    val_loader = DataLoader(val_set, batch_size=batch_size, shuffle=False, num_workers=num_workers, pin_memory=True)
    return train_loader, val_loader

# ---------- 1 epoch„ÅÆÂ≠¶Áøí ----------
def train_one_epoch(model, loader, optimizer, device, criterion):
    model.train()
    total_loss, correct, total = 0.0, 0, 0
    for inputs, targets in loader:
        inputs, targets = inputs.to(device), targets.to(device)

        optimizer.zero_grad()
        outputs = model(inputs)
        loss = criterion(outputs, targets)
        loss.backward()
        optimizer.step()

        total_loss += loss.item() * inputs.size(0)
        _, preds = torch.max(outputs, 1)
        correct += (preds == targets).sum().item()
        total += targets.size(0)

    return total_loss / total, correct / total

# ---------- Ê§úË®º ----------
def validate(model, loader, device, criterion):
    model.eval()
    total_loss, correct, total = 0.0, 0, 0
    with torch.no_grad():
        for inputs, targets in loader:
            inputs, targets = inputs.to(device), targets.to(device)
            outputs = model(inputs)
            loss = criterion(outputs, targets)

            total_loss += loss.item() * inputs.size(0)
            _, preds = torch.max(outputs, 1)
            correct += (preds == targets).sum().item()
            total += targets.size(0)

    return total_loss / total, correct / total

# ---------- „É°„Ç§„É≥„É´„Éº„Éó ----------
def main():
    data_dir = './sushi_dataset'  # train/val „ÅÆ„Éá„Éº„Çø„ÅåÂÖ•„Å£„Åü„Éá„Ç£„É¨„ÇØ„Éà„É™
    num_classes = 290
    epochs = 100
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')

    model = build_model(num_classes).to(device)
    criterion = nn.CrossEntropyLoss()
    optimizer = get_optimizer(model)
    scheduler = StepLR(optimizer, step_size=20, gamma=0.5)

    train_loader, val_loader = get_dataloaders(data_dir)

    best_acc = 0.0
    for epoch in range(epochs):
        print(f"\nEpoch {epoch+1}/{epochs}")
        unfreeze_layers(model, epoch)

        train_loss, train_acc = train_one_epoch(model, train_loader, optimizer, device, criterion)
        val_loss, val_acc = validate(model, val_loader, device, criterion)
        scheduler.step()

        print(f"Train Loss: {train_loss:.4f}  |  Acc: {train_acc:.2%}")
        print(f"Val   Loss: {val_loss:.4f}  |  Acc: {val_acc:.2%}")

        if val_acc > best_acc:
            best_acc = val_acc
            torch.save(model.state_dict(), 'best_model.pth')
            print(f"‚úÖ Best model saved (val_acc={val_acc:.2%})")

if __name__ == '__main__':
    main()



„Éº„Éº„Éº„Éº„Éº„Éº„Éº„Éº„Éº„Éº„Éº„Éº„Éº„Éº„Éº„Éº„Éº„Éº„Éº„Éº„Éº„Éº„Éº„Éº„Éº„Éº„Éº„Éº„Éº„Éº„Éº„Éº„Éº„Éº„Éº„Éº
class EarlyStopping:
    def __init__(self, patience=15, verbose=False):
        self.patience = patience
        self.counter = 0
        self.best_score = None
        self.early_stop = False
        self.verbose = verbose
        self.best_model = None

    def __call__(self, val_acc, model):
        score = val_acc

        if self.best_score is None:
            self.best_score = score
            self.best_model = model.state_dict()
        elif score <= self.best_score:
            self.counter += 1
            if self.verbose:
                print(f"EarlyStopping counter: {self.counter}/{self.patience}")
            if self.counter >= self.patience:
                self.early_stop = True
        else:
            self.best_score = score
            self.best_model = model.state_dict()
            self.counter = 0


def main():
    data_dir = './sushi_dataset'
    num_classes = 290
    epochs = 300
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')

    model = build_model(num_classes).to(device)
    criterion = nn.CrossEntropyLoss()
    optimizer = get_optimizer(model)
    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=5, verbose=True)

    train_loader, val_loader = get_dataloaders(data_dir)

    early_stopping = EarlyStopping(patience=15, verbose=True)
    for epoch in range(epochs):
        print(f"\nEpoch {epoch+1}/{epochs}")
        unfreeze_layers(model, epoch)

        train_loss, train_acc = train_one_epoch(model, train_loader, optimizer, device, criterion)
        val_loss, val_acc = validate(model, val_loader, device, criterion)

        scheduler.step(val_loss)  # loss„ÅßË™øÊï¥

        print(f"Train Loss: {train_loss:.4f}  |  Acc: {train_acc:.2%}")
        print(f"Val   Loss: {val_loss:.4f}  |  Acc: {val_acc:.2%}")

        # EarlyStoppingÁõ£Ë¶ñ
        early_stopping(val_acc, model)
        if early_stopping.early_stop:
            print("üõë Early stopping triggered.")
            break

    # ÁµÇ‰∫ÜÂæå„ÄÅÊúÄÈ´ò„É¢„Éá„É´‰øùÂ≠ò
    torch.save(early_stopping.best_model, 'best_model.pth')
    print("‚úÖ Best model saved after EarlyStopping.")
